# üîó Guia de Integra√ß√£o Backend-Frontend

## üìã Vis√£o Geral

Integra√ß√£o do frontend **cognit-ai-web** (React + TypeScript) com backend **cognit-ai** (FastAPI + Python) em produ√ß√£o.

### Estrutura
```
Backend: https://cognit-ai-s3q92.ondigitalocean.app (Produ√ß√£o)
Frontend: /home/iebt/projects/cognit-ai-web (Local)
Docs: https://cognit-ai-s3q92.ondigitalocean.app/docs
```

## üéØ Status

### ‚úÖ Backend (Produ√ß√£o)
- **URL**: https://cognit-ai-s3q92.ondigitalocean.app
- **Autentica√ß√£o Google OAuth**: Funcionando
- **Chat IA**: OpenAI + Anthropic + streaming
- **Modelos LLM**: 7 modelos dispon√≠veis
- **Docs**: /docs endpoint ativo

### ‚úÖ Frontend (Local)
- **100% funcional**: Dados mockados
- **Pronto**: Estrutura de APIs definida

## üöÄ Integra√ß√µes Poss√≠veis

### 1. ‚úÖ **AUTENTICA√á√ÉO GOOGLE OAUTH** (Pronto)

#### Backend Dispon√≠vel
```python
# Endpoints funcionais
GET  /api/v1/auth/google/login     # Gera URL de autoriza√ß√£o
GET  /api/v1/auth/google/callback  # Processa callback do Google
GET  /api/v1/auth/me               # Informa√ß√µes do usu√°rio atual
POST /api/v1/auth/logout           # Logout
```

#### Frontend Atual (Mock)
```typescript
// Endpoints esperados pelo frontend
POST /auth/google      # Autentica√ß√£o Google (token-based)
GET  /auth/profile     # Perfil do usu√°rio
POST /auth/logout      # Logout
```

#### üîß Adapta√ß√£o Necess√°ria
O backend usa **fluxo redirect-based** enquanto o frontend espera **token-based**. Duas op√ß√µes:

**Op√ß√£o A**: Adaptar frontend para redirect-based (recomendado)
**Op√ß√£o B**: Implementar endpoint token-based no backend

### 2. ‚úÖ **CHAT COM IA** (Pronto)

#### Backend Dispon√≠vel
```python
POST /api/v1/chat/
# Request: { messages: [{"role": "user", "content": "..."}], model: "gpt-4o-mini" }
# Response: { message: "...", role: "ai" }
# Suporte: OpenAI, Anthropic, streaming
```

#### Frontend Atual (Mock)
```typescript
POST /chat/message
# FormData: content, provider, model, files
# Response: Message object com tokens, cost, etc.
```

#### üîß Adapta√ß√£o Necess√°ria
- Mapear estrutura de request/response
- Implementar streaming no frontend
- Ajustar formato de dados

### 3. ‚úÖ **MODELOS LLM** (Pronto)

#### Backend Dispon√≠vel
```python
GET /api/v1/llm-models/           # Lista todos os modelos
GET /api/v1/llm-models/{id}       # Detalhes de modelo espec√≠fico
# Seeders com: OpenAI, Anthropic, Google models
```

#### Frontend Atual (Mock)
```typescript
GET /chat/providers  # Lista de providers
GET /chat/models     # Lista de modelos por provider
```

#### üîß Adapta√ß√£o Necess√°ria
- Mapear estrutura de dados
- Agrupar modelos por provider no frontend

## ‚ö†Ô∏è Funcionalidades Parciais

### 4. üî∂ **GEST√ÉO DE CONVERSAS** (Backend precisa expans√£o)

#### Backend Atual
```python
# Modelos existem no banco
class ChatHistorySession(Base):
    id, user_id, title, first_interaction, last_interaction

class ChatHistoryMessage(Base):
    id, session_id, author, content, timestamp
```

#### Endpoints Necess√°rios
```python
# Implementar no backend
GET  /api/v1/chat/conversations      # Listar conversas
POST /api/v1/chat/conversations      # Criar conversa
GET  /api/v1/chat/conversations/{id} # Obter conversa espec√≠fica
PUT  /api/v1/chat/conversations/{id} # Atualizar conversa
DEL  /api/v1/chat/conversations/{id} # Deletar conversa
```

### 5. üî∂ **PERSIST√äNCIA DE MENSAGENS** (Implementa√ß√£o autom√°tica)

#### Necess√°rio
- Salvar mensagens automaticamente no endpoint de chat
- Associar mensagens a sess√µes/conversas
- Implementar hist√≥rico de mensagens

## ‚ùå Funcionalidades N√£o Implementadas

### 6. **SISTEMA DE WORKSPACES**
O frontend possui sistema completo de workspaces, projetos e documentos que n√£o existe no backend.

### 7. **AGENTES IA ESPECIALIZADOS**
O frontend possui agentes especializados que n√£o existem no backend.

### 8. **UPLOAD E GEST√ÉO DE ARQUIVOS**
O frontend suporta upload de arquivos, mas o backend n√£o possui endpoints correspondentes.

## üõ†Ô∏è Configura√ß√£o

### Backend
‚úÖ **J√° rodando em produ√ß√£o** - N√£o precisa configurar

### Frontend
```bash
cd /home/iebt/projects/cognit-ai-web

# Configurar ambiente
cat > .env.local << EOF
VITE_USE_MOCK_SERVER=false
VITE_API_BASE_URL=https://cognit-ai-s3q92.ondigitalocean.app
VITE_GOOGLE_CLIENT_ID=1034043274094-vj8odt69hbpevp3uv9nl65ctf24r8km0.apps.googleusercontent.com
EOF

# Iniciar desenvolvimento
npm run dev
```

## üìä Mapeamento de Dados

### Usu√°rio
```typescript
// Backend
{
  id: number,
  google_sub: string,
  email: string,
  name: string
}

// Frontend esperado
{
  id: string,
  email: string,
  name: string,
  role: 'admin' | 'user' | 'viewer',
  avatar?: string,
  // ... outros campos
}
```

### Mensagem
```typescript
// Backend
{
  message: string,
  role: "ai"
}

// Frontend esperado
{
  id: string,
  content: string,
  role: 'user' | 'assistant' | 'system',
  timestamp: string,
  conversationId: string,
  provider?: string,
  model?: string,
  tokens?: { prompt: number, completion: number, total: number },
  cost?: number
}
```

### Modelo LLM
```typescript
// Backend
{
  id: number,
  name: string,
  provider: string,
  is_active: boolean,
  max_tokens?: number,
  context_window?: number,
  price_usd?: number
}

// Frontend esperado (agrupado por provider)
{
  id: string,
  name: string,
  description: string,
  isAvailable: boolean,
  models: LLMModel[]
}
```

## üéØ Plano de Implementa√ß√£o

### Fase 1: Integra√ß√µes B√°sicas (1-2 dias)
1. **Configurar ambientes** (backend + frontend)
2. **Integrar autentica√ß√£o Google OAuth**
3. **Conectar chat b√°sico**
4. **Mapear modelos LLM**

### Fase 2: Funcionalidades Avan√ßadas (3-5 dias)
1. **Implementar gest√£o de conversas no backend**
2. **Adicionar persist√™ncia de mensagens**
3. **Conectar hist√≥rico com frontend**

### Fase 3: Expans√µes (1-2 semanas)
1. **Sistema de workspaces** (se necess√°rio)
2. **Upload de arquivos** (se necess√°rio)
3. **Agentes especializados** (se necess√°rio)

## üîç Debugging e Testes

### Verificar Backend
```bash
# Testar endpoints
curl http://localhost:8501/health
curl http://localhost:8501/api/v1/llm-models/

# Logs do servidor
python api.py  # Ver logs no terminal
```

### Verificar Frontend
```bash
# Desabilitar mock server
# Em src/api/mockServer.ts, linha 425:
export const shouldUseMockServer = (): boolean => {
  return false;  // Mudan√ßa aqui
};

# Verificar network tab no browser
# Todas as chamadas devem ir para localhost:8501
```

## üìö Recursos Adicionais

### Documenta√ß√£o Backend
- **FastAPI Docs**: http://localhost:8501/docs (quando rodando)
- **Modelos**: `/home/iebt/projects/cognit-ai/database/models/`
- **Routers**: `/home/iebt/projects/cognit-ai/src/presentation/api/routers/`

### Documenta√ß√£o Frontend
- **APIs**: `/home/iebt/projects/cognit-ai-web/src/api/`
- **Types**: `/home/iebt/projects/cognit-ai-web/src/types/`
- **Mock Data**: `/home/iebt/projects/cognit-ai-web/src/api/mock/`

## ‚ö° Quick Start

Para uma integra√ß√£o r√°pida, siga estes passos:

1. **Configure o backend** (5 min)
2. **Desabilite o mock server** (1 min)
3. **Teste autentica√ß√£o** (10 min)
4. **Teste chat b√°sico** (10 min)
5. **Ajuste mapeamento de dados** (30 min)

Total: ~1 hora para integra√ß√£o b√°sica funcional.
